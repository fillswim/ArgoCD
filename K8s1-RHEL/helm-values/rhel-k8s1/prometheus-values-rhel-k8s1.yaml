# =================================================================================================
#                                               Global
# =================================================================================================

## Deploy a Prometheus instance
prometheus:

  enabled: true

  # service:
  #   port: 9090
  #   targetPort: 9090
  #   loadBalancerIP: 192.168.2.226
  #   type: LoadBalancer

  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: ca-cluster-issuer
    hosts:
      - prometheus-rhel-k8s1.fillswim.local
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus-rhel-k8s1.fillswim.local

  prometheusSpec:

    overrideHonorLabels: true
    overrideHonorTimestamps: true
    allowOverpassTLS: true
    allowOverpassRemoteWriteHeaders: true

    externalLabels:
      cluster: rhel-k8s1

    remoteWrite:
      - url: https://mimir.fillswim.local/api/v1/push
        headers:
          X-Scope-OrgID: "fillswim"
        tlsConfig:
          insecureSkipVerify: true

        # writeRelabelConfigs:
        #   - sourceLabels: [__name__]
        #     regex: "^node_cpu_seconds_total$"
        #     action: keep

        #   - sourceLabels: [__name__]
        #     regex: "^prometheus_remote_storage_.*|^prometheus_tsdb_.*|^prometheus_engine_.*|^up$|^scrape_.*"
        #     action: keep

        #   - action: drop


        writeRelabelConfigs:
          - sourceLabels: [__name__]
            regex: "node_cpu_seconds_total|node_uname_info"
            action: keep

        # writeRelabelConfigs:
        # - sourceLabels:
        #   - "__name__"
        #   # regex: ":node_memory_MemAvailable_bytes:sum|alertmanager_alerts|alertmanager_alerts_invalid_total|alertmanager_alerts_received_total|alertmanager_notification_latency_seconds_bucket|alertmanager_notification_latency_seconds_count|alertmanager_notification_latency_seconds_sum|alertmanager_notifications_failed_total|alertmanager_notifications_total|apiserver_request:availability30d|apiserver_request_total|cluster_quantile:apiserver_request_duration_seconds:histogram_quantile|code_resource:apiserver_request_total:rate5m|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|coredns_cache_entries|coredns_cache_hits_total|coredns_cache_misses_total|coredns_cache_size|coredns_dns_do_requests_total|coredns_dns_request_count_total|coredns_dns_request_do_count_total|coredns_dns_request_duration_seconds_bucket|coredns_dns_request_size_bytes_bucket|coredns_dns_request_type_count_total|coredns_dns_requests_total|coredns_dns_response_rcode_count_total|coredns_dns_response_size_bytes_bucket|coredns_dns_responses_total|etcd_disk_backend_commit_duration_seconds_bucket|etcd_disk_wal_fsync_duration_seconds_bucket|etcd_mvcc_db_total_size_in_bytes|etcd_network_client_grpc_received_bytes_total|etcd_network_client_grpc_sent_bytes_total|etcd_network_peer_received_bytes_total|etcd_network_peer_sent_bytes_total|etcd_server_has_leader|etcd_server_leader_changes_seen_total|etcd_server_proposals_applied_total|etcd_server_proposals_committed_total|etcd_server_proposals_failed_total|etcd_server_proposals_pending|go_goroutines|grpc_server_handled_total|grpc_server_started_total|instance:node_cpu_utilisation:rate5m|instance:node_load1_per_cpu:ratio|instance:node_memory_utilisation:ratio|instance:node_network_receive_bytes_excluding_lo:rate5m|instance:node_network_receive_drop_excluding_lo:rate5m|instance:node_network_transmit_bytes_excluding_lo:rate5m|instance:node_network_transmit_drop_excluding_lo:rate5m|instance:node_num_cpu:sum|instance:node_vmstat_pgmajfault:rate5m|instance_device:node_disk_io_time_seconds:rate5m|instance_device:node_disk_io_time_weighted_seconds:rate5m|kube_node_status_allocatable|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_info|kube_pod_owner|kube_resourcequota|kube_statefulset_metadata_generation|kube_statefulset_replicas|kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_statefulset_status_replicas_updated|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_duration_seconds_bucket|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubeproxy_network_programming_duration_seconds_bucket|kubeproxy_network_programming_duration_seconds_count|kubeproxy_sync_proxy_rules_duration_seconds_bucket|kubeproxy_sync_proxy_rules_duration_seconds_count|namespace_cpu:kube_pod_container_resource_limits:sum|namespace_cpu:kube_pod_container_resource_requests:sum|namespace_memory:kube_pod_container_resource_limits:sum|namespace_memory:kube_pod_container_resource_requests:sum|namespace_workload_pod|namespace_workload_pod:kube_pod_owner:relabel|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_exporter_build_info|node_filesystem_avail_bytes|node_filesystem_size_bytes|node_load1|node_load15|node_load5|node_memory_Buffers_bytes|node_memory_Cached_bytes|node_memory_MemAvailable_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|node_namespace_pod_container:container_memory_cache|node_namespace_pod_container:container_memory_rss|node_namespace_pod_container:container_memory_swap|node_namespace_pod_container:container_memory_working_set_bytes|node_network_receive_bytes_total|node_network_transmit_bytes_total|process_cpu_seconds_total|process_resident_memory_bytes|process_start_time_seconds|prometheus|prometheus_build_info|prometheus_engine_query_duration_seconds|prometheus_engine_query_duration_seconds_count|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_tsdb_head_chunks|prometheus_tsdb_head_samples_appended_total|prometheus_tsdb_head_series|rest_client_request_duration_seconds_bucket|rest_client_requests_total|scheduler_binding_duration_seconds_bucket|scheduler_binding_duration_seconds_count|scheduler_e2e_scheduling_duration_seconds_bucket|scheduler_e2e_scheduling_duration_seconds_count|scheduler_scheduling_algorithm_duration_seconds_bucket|scheduler_scheduling_algorithm_duration_seconds_count|scheduler_volume_scheduling_duration_seconds_bucket|scheduler_volume_scheduling_duration_seconds_count|storage_operation_duration_seconds_bucket|storage_operation_duration_seconds_count|storage_operation_errors_total|up|volume_manager_total_volumes|workqueue_adds_total|workqueue_depth|workqueue_queue_duration_seconds_bucket"
        #   regex: "node_cpu_seconds_total"
        #   action: "keep"

        # writeRelabelConfigs:
        #   # Keep only critical metrics
        #   - sourceLabels: [__name__]
        #     regex: ^(up|kube_pod_|kube_node_|prometheus_remote_storage_.+)$
        #     action: keep
        #   - action: drop    # дропаем всё остальное

        # writeRelabelConfigs:
        #   # 1. Убрать почти все kubelet/cAdvisor контейнерные метрики (очень жирные)
        #   - sourceLabels: [__name__]
        #     regex: ^(container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_.+|container_fs_.+)
        #     action: drop

        #   # 2. Убрать kube-state-metrics шум (оставляем только самое важное)
        #   - sourceLabels: [__name__]
        #     regex: ^(kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_status_phase|kube_pod_container_status_ready|.+) 
        #     action: drop
        #   - sourceLabels: [__name__]
        #     regex: ^(kube_pod_status_phase|kube_pod_container_status_ready|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready)
        #     action: keep

        #   # 3. Убрать все scrape-метрики самого Prometheus (up, scrape_duration, etc.)
        #   - sourceLabels: [job]
        #     regex: (prometheus|kube-state-metrics|node-exporter)
        #     action: drop

          # # 4. Полностью отключить отправку exemplars (экономия до 20 %)
          # - action: drop
          #   regex: .+
          #   targetLabel: __name__
          #   sourceLabels: [__name__]
          #   action: drop
          #   # или проще — добавить в remote_write:
          #   # sendExemplars: false

        # Настройки надёжной доставки
        sendExemplars: true
        queueConfig:
          capacity: 50000
          maxShards: 50
          minShards: 5
          maxSamplesPerSend: 2000  # меньше батчи → меньше 429
          batchSendDeadline: 10s
          minBackoff: 500ms
          maxBackoff: 30s

    # Чтобы Prometheus локально не хранил много (он становится "агентом")
    retention: 2h
    retentionSize: "2GB"

    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
    enableAdminAPI: true

    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
    ## with the new list of secrets.
    secrets:
      - etcd-client-cert

    # 2025-10-23: Добавлено для сохранения данных в PersistentVolumeClaim
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi

    ## 2025-10-23: Задает на какой период времени хранить данные в метрик
    ## How long to retain metrics
    # retention: 15d
    ## Maximum size of metrics
    ## Unit format should be in the form of "50GiB"
    # retentionSize: "10GiB"
    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    scrapeInterval: "30s"
    ## Interval between consecutive evaluations.
    evaluationInterval: "30s"

    ## Resource limits & requests
    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m

# =================================================================================================
#                                               kubeEtcd
# =================================================================================================

## Component scraping etcd
kubeEtcd:
  enabled: true

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  service:
    enabled: true
    port: 2381
    targetPort: 2379
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  serviceMonitor:
    scheme: https
    insecureSkipVerify: false
    serverName: localhost
    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    certFile: /etc/prometheus/secrets/etcd-client-cert/client.crt
    keyFile: /etc/prometheus/secrets/etcd-client-cert/client.key

# =================================================================================================
#                                            Alertmanager
# =================================================================================================

# 2025-10-23: Добавлено для развертывания Alertmanager
alertmanager:
  alertmanagerSpec:
    replicas: 3
    retention: 120h # 5 days

    ## Storage is the definition of how storage will be used by the ThanosRuler instances.
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    ## Resource limits & requests
    resources:
      requests:
        memory: 400Mi
        cpu: 100m
      limits:
        memory: 800Mi
        cpu: 200m

# =================================================================================================
#                                              Grafana
# =================================================================================================
grafana:
  enabled: false