# =================================================================================================
#                                               Global
# =================================================================================================

## Deploy a Prometheus instance
prometheus:

  enabled: true

  # service:
  #   port: 9090
  #   targetPort: 9090
  #   loadBalancerIP: 192.168.2.226
  #   type: LoadBalancer

  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: ca-cluster-issuer
    hosts:
      - prometheus-rhel-k8s1.fillswim.local
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus-rhel-k8s1.fillswim.local

  prometheusSpec:

    overrideHonorLabels: true
    overrideHonorTimestamps: true
    allowOverpassTLS: true
    allowOverpassRemoteWriteHeaders: true

    externalLabels:
      cluster: rhel-k8s1

    remoteWrite:
      - url: https://mimir.fillswim.local/api/v1/push
        headers:
          X-Scope-OrgID: "fillswim"
        tlsConfig:
          insecureSkipVerify: true

        writeRelabelConfigs:
          - sourceLabels: [__name__]
            regex: "node_cpu_seconds_total"
            action: keep

        # writeRelabelConfigs:
        #   # Keep only critical metrics
        #   - sourceLabels: [__name__]
        #     regex: ^(up|kube_pod_|kube_node_|prometheus_remote_storage_.+)$
        #     action: keep
        #   - action: drop    # дропаем всё остальное

        # writeRelabelConfigs:
        #   # 1. Убрать почти все kubelet/cAdvisor контейнерные метрики (очень жирные)
        #   - sourceLabels: [__name__]
        #     regex: ^(container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_network_.+|container_fs_.+)
        #     action: drop

        #   # 2. Убрать kube-state-metrics шум (оставляем только самое важное)
        #   - sourceLabels: [__name__]
        #     regex: ^(kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_status_phase|kube_pod_container_status_ready|.+) 
        #     action: drop
        #   - sourceLabels: [__name__]
        #     regex: ^(kube_pod_status_phase|kube_pod_container_status_ready|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready)
        #     action: keep

        #   # 3. Убрать все scrape-метрики самого Prometheus (up, scrape_duration, etc.)
        #   - sourceLabels: [job]
        #     regex: (prometheus|kube-state-metrics|node-exporter)
        #     action: drop

          # # 4. Полностью отключить отправку exemplars (экономия до 20 %)
          # - action: drop
          #   regex: .+
          #   targetLabel: __name__
          #   sourceLabels: [__name__]
          #   action: drop
          #   # или проще — добавить в remote_write:
          #   # sendExemplars: false

        # Настройки надёжной доставки
        sendExemplars: true
        queueConfig:
          capacity: 50000
          maxShards: 50
          minShards: 5
          maxSamplesPerSend: 2000  # меньше батчи → меньше 429
          batchSendDeadline: 10s
          minBackoff: 500ms
          maxBackoff: 30s

    # Чтобы Prometheus локально не хранил много (он становится "агентом")
    retention: 2h
    retentionSize: "2GB"

    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
    enableAdminAPI: true

    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
    ## with the new list of secrets.
    secrets:
      - etcd-client-cert

    # 2025-10-23: Добавлено для сохранения данных в PersistentVolumeClaim
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi

    ## 2025-10-23: Задает на какой период времени хранить данные в метрик
    ## How long to retain metrics
    # retention: 15d
    ## Maximum size of metrics
    ## Unit format should be in the form of "50GiB"
    # retentionSize: "10GiB"
    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    scrapeInterval: "30s"
    ## Interval between consecutive evaluations.
    evaluationInterval: "30s"

    ## Resource limits & requests
    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m

# =================================================================================================
#                                               kubeEtcd
# =================================================================================================

## Component scraping etcd
kubeEtcd:
  enabled: true

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  service:
    enabled: true
    port: 2381
    targetPort: 2379
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  serviceMonitor:
    scheme: https
    insecureSkipVerify: false
    serverName: localhost
    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    certFile: /etc/prometheus/secrets/etcd-client-cert/client.crt
    keyFile: /etc/prometheus/secrets/etcd-client-cert/client.key

# =================================================================================================
#                                            Alertmanager
# =================================================================================================

# 2025-10-23: Добавлено для развертывания Alertmanager
alertmanager:
  alertmanagerSpec:
    replicas: 3
    retention: 120h # 5 days

    ## Storage is the definition of how storage will be used by the ThanosRuler instances.
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    ## Resource limits & requests
    resources:
      requests:
        memory: 400Mi
        cpu: 100m
      limits:
        memory: 800Mi
        cpu: 200m

# =================================================================================================
#                                              Grafana
# =================================================================================================
grafana:
  enabled: false